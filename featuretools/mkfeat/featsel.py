from sklearn.feature_selection import VarianceThreshold
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import f_regression

import pandas as pd
from pandas import DataFrame

from featuretools.selection import (remove_highly_correlated_features, remove_single_value_features)

from .elapsed_time import ElapsedTime
from .progress_phase import ProgressPhase

THRESHOLD = 0.02


def select_features(df: DataFrame, features, n_feats,
                    df_label: DataFrame, df_train: DataFrame, proghandler, elapsed_time: ElapsedTime):
    """
    Select features based on feature variance or score against label(if any)

    Args:
        df: feature matrix generated by dfs
        features: features generated by dfs
        n_feats: number of features to select
        df_label: labels. It can be None
        df_train: indication vector(dataframe) for train data. It can be None
        elapsed_time: elapsed time marker

    Returns:

    """

    df, features = remove_single_value_features(df, features, count_nan_as_value=True)
    proghandler(100, ProgressPhase.REMOVE_SINGLE)
    elapsed_time.mark()

    df, features = _remove_correlated_features(df, features)
    proghandler(100, ProgressPhase.REMOVE_CORREL)
    elapsed_time.mark()

    df_test = None
    labels = None
    if df_train is not None:
        colname_train = df_train.columns[0]
        df = df.join(df_train)
        if df_label is not None:
            df = df.join(df_label)
        df_train = df[df[colname_train] == True]
        df_test = df[df[colname_train] == False]
        if df_label is not None:
            labels = df_train[df_label.columns[0]].values
            df_train.drop(columns=df_label.columns[0], axis=1, inplace=True)
            df_test.drop(columns=df_label.columns[0], axis=1, inplace=True)

        df_train.drop(columns=colname_train, axis=1, inplace=True)
        df_test.drop(columns=colname_train, axis=1, inplace=True)
        df = df_train
    else:
        if df_label is not None:
            labels = df_label.iloc[:, 0].values

    if labels is not None:
        _select_highscore_features(df, df_test, features, labels, n_feats)
        if df_test is not None:
            df = pd.concat([df, df_test])
    else:
        _select_highvar_features(df, df_test, features, n_feats)

    proghandler(100, ProgressPhase.SELECT_BEST)
    return df, features


def _remove_correlated_features(df, features):
    f_names_check = []
    for feat in features:
        name = feat.get_name()
        if feat.variable_type.type_string == "numeric" or feat.variable_type.type_string == "boolean":
            f_names_check.append(name)
    df_new, features_new = remove_highly_correlated_features(df, features, features_to_check=f_names_check)
    return df_new, features_new


def _select_highscore_features(df: DataFrame, df_test, features, labels, n_feats):
    """
    Select highest scored features. This selection scheme is used for the data with label

    Args:
        df: dataframe to select
        df_test: dataframe with test data
        features: features array
        labels: labels
        n_feats: Number of features to select
    """

    colname_scores = []
    for feat in features:
        name = feat.get_name()
        if feat.variable_type.type_string == "numeric" or feat.variable_type.type_string == "boolean":
            arr = df[name].values.reshape(len(df), 1)
            try:
                f_score, pval = f_regression(arr, labels)
                score = f_score[0]
            except ValueError:
                score = 0
        else:
            score = 100000
        colname_scores.append((name, score))

    _select_df_features(df, df_test, features, colname_scores, n_feats)


def _select_highvar_features(df: DataFrame, df_test, features, n_feats):
    """
    Select highest variant features. This selection scheme is used for the data with no label

    Args:
        df: dataframe to select
        df_test: dataframe with test data
        features: features array
        n_feats: Number of features to select
    """

    colname_vars = []
    for feat in features:
        name = feat.get_name()
        if feat.variable_type.type_string == "numeric" or feat.variable_type.type_string == "boolean":
            sel = VarianceThreshold()
            scaler = MinMaxScaler()
            try:
                arr = df[name].values.reshape(len(df), 1)
                scaler.fit(arr)
                sel.fit(scaler.transform(arr))
                var = sel.variances_[0]
            except ValueError:
                var = 0
        else:
            var = 1
        colname_vars.append((name, var))

    _select_df_features(df, df_test, features, colname_vars, n_feats)


def _select_df_features(df: DataFrame, df_test: DataFrame, features, colname_vals, n_feats, threshold=None):
    colnames_selected = []

    for colname_val in sorted(colname_vals, key=lambda x: x[1], reverse=True):
        if threshold is not None:
            if colname_val[1] < threshold:
                break
        colnames_selected.append(colname_val[0])
        if len(colnames_selected) >= n_feats:
            break

    for idx in range(len(df.columns) - 1, -1, -1):
        colname = df.columns[idx]
        if colname not in colnames_selected:
            df.drop(columns=colname, axis=1, inplace=True)
            if df_test is not None:
                df_test.drop(columns=colname, axis=1, inplace=True)
            features.pop(idx)
